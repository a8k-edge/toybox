{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from langchain docs: https://python.langchain.com/docs/get_started/quickstart\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# See models - https://python.langchain.com/docs/integrations/chat/\n",
    "# Note: not all the models interact with system messages in the same way!\n",
    "# You have to learn about how the specific model you are interested in behaves.\n",
    "\n",
    "# create the template/format\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language} with many years of experience teaching beginner language students.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inp=\"English\", out=\"German\", text=\"I love programming?\"):\n",
    "    response = chat(\n",
    "        chat_prompt.format_messages(input_language=inp, output_language=out, text=text)\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "\n",
    "translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic approaches to prompting\n",
    "\n",
    "There are many variants on prompt engineering research over the last two years. \n",
    "\n",
    "| Title | Medium | Organizations | Date | Code | Method |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/pdf/2110.08387.pdf) | Paper | U Washington, Allen AI | September 2022 | https://github.com/liujch1998/GKP |\n",
    "| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | Paper | Google Brain | January 2023 | | Chain of Thought |\n",
    "| [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) | Paper | CMU | January 2023 |  | \n",
    "| [Chain of Thought Paradigms in LLMs](https://matt-rickard.com/chain-of-thought-in-llms) | Blog |  | March 2023 |  | Chain of Thought |\n",
    "| [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)| Paper| Google Brain, Princeton| March 2023| | ReAct |\n",
    "| [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) | Paper| Google | March 2023| | |\n",
    "| [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/pdf/2303.09014.pdf) | Paper | U Washington, Microsoft, UC Irvine, Allen AI | March 2023 | | Automatic Reasoning and Tool Use |\n",
    "| [Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/pdf/2205.10625.pdf) | Paper | Google Brain | April 2023 |  | \n",
    "| [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/pdf/2210.03350.pdf) | Paper | U Washington, MosaicML, Meta AI, Allen AI | May 2023 | https://github.com/ofirpress/self-ask | Self-ask Prompting | \n",
    "| [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://arxiv.org/pdf/2305.08845.pdf) | Paper | Tencent | May 2023 |  | \n",
    "| [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) | Paper | Deepmind, Princeton | May 2023 | https://github.com/princeton-nlp/tree-of-thought-llm | Chain of Thought |\n",
    "| [Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html) | Blog | KDNuggets | July 2023 |  | Chain of Thought | \n",
    "| [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/pdf/2309.04269.pdf) | Paper | Salesforce, Columbia, MIT | September 2023 |  | Chain of Density | \n",
    "| [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution](https://arxiv.org/abs/2309.16797?fbclid=IwAR1o-VI0DSwNOawBAQAcv0adoDakSWrgwPuLxWqJhLdCbouuZBA0Gm7Sy8I) | Paper | Deepmind | October 2023 |  | Evolutionary Algorithm |  \n",
    "| [Prompt Design and Engineering: Introduction and Advanced Methods](https://arxiv.org/abs/2401.14423) | Paper | Independent | January 2024 |  | |  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
