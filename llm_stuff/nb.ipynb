{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "import cohere\n",
    "import ai21\n",
    "\n",
    "from transformers import AutoModel, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = ...\n",
    "cohere_key = ...\n",
    "ai21_key = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"How is generative AI affecting the infrastrucutre machine learning developers need access to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai_key)\n",
    "gpt35_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": PROMPT}]\n",
    ")\n",
    "gpt35_text_response = gpt35_completion.to_dict()[\"choices\"][0][\"message\"][\n",
    "    \"content\"\n",
    "].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(cohere_key)\n",
    "cohere_cmd_completion = co.generate(prompt=PROMPT, model=\"command\")\n",
    "cohere_cmd_response = cohere_cmd_completion.generations[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohere_cmd_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai21 import AI21Client\n",
    "\n",
    "client = AI21Client(api_key=ai21_key)\n",
    "response_mid = client.completion.create(\n",
    "    model=\"j2-mid\",\n",
    "    prompt=PROMPT,\n",
    "    num_results=1,\n",
    "    max_tokens=100,\n",
    "    temperature=0.4,\n",
    "    top_k_return=0,\n",
    "    top_p=1,\n",
    "    stop_sequences=[\"##\"],\n",
    ")\n",
    "jurassic2_response = response_mid.completions[0].data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\n",
    "    f\"\"\"\n",
    "\n",
    "**OpenAI GPT3.5**: {gpt35_text_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**Cohere Command**: {cohere_cmd_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**AI21 Jurassic2**: {jurassic2_response}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: You can make each of these models do almost anything, making this all muddy. <br/>The point of this table is to highlight which of these APIs have documented endpoints for certain tasks.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Endpoint / API | OpenAI | Cohere | Claude | A21 |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| Prompt-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Chat-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Text embeddings | ✅ | ✅ | ❌ | ✅ |\n",
    "| Fine-tuning | ✅ | ✅ | ❌ | ✅ |\n",
    "| Language detection | ❌ | ✅ | ❌ | ❌ |\n",
    "| Raw document processing | ✅ | ✅ | ❌ | ✅ | \n",
    "| Rerank / document relevance | ❌ | ✅ | ❌ | ✅ |\n",
    "| Text/image to image | ✅ | ❌ | ❌ | ❌ |\n",
    "| Audio-to-text | ✅ | ❌ | ❌ | ❌ |\n",
    "| Moderations / toxicitiy | ✅ | ✅ | ❌ | ❌ | \n",
    "\n",
    "</center>\n",
    "\n",
    "Some opinions related to this table:\n",
    "- If you want to pay for the **best chat model** --> \n",
    "    - OpenAI's GPT4 API is the current gold standard.\n",
    "- If you want **multimodal** --> \n",
    "    - OpenAI APIs are great - [Images](https://platform.openai.com/docs/api-reference/images), [Audio](https://platform.openai.com/docs/api-reference/audio)\n",
    "    -  Stability AI has some nice products not listed here. \n",
    "    - Check out this recent survey paper: [The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)](https://arxiv.org/abs/2309.17421)\n",
    "- If you care a lot about **content moderation** --> \n",
    "    - [Cohere](https://docs.cohere.com/docs/content-moderation-with-classify) and [OpenAI](https://platform.openai.com/docs/api-reference/moderations) have the most API support\n",
    "- If you want fine-grained **multi-lingual models** --> \n",
    "    - Try Cohere's [Multilingual Embedding](https://docs.cohere.com/docs/multilingual-language-models) APIs\n",
    "- If you want a model that can **analyze grammar** --> \n",
    "    - Try A21's [Text Improvements](https://docs.ai21.com/reference/text-improvements-api-ref) and [Grammatical Error Corrections](https://docs.ai21.com/reference/gec-api-ref) APIs\n",
    "- The public Claude product is a personal favorite, however their API access and feature support is lacking behind others in this list\n",
    "\n",
    "Of course, you can also try meshing them together if you have the budget and engineering will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 paper: https://arxiv.org/pdf/2210.11416.pdf\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more about transformers like the BERT and GPT family and how they work? \n",
    "- Sebastian Raschka recently gave his description of the history of the transformer in this concise [post](https://www.linkedin.com/posts/sebastianraschka_llms-largelanguagemodels-ai-activity-7121484400701186048--47t?utm_source=share&utm_medium=member_desktop).\n",
    "- Check out the amazing [Bertviz](https://github.com/jessevig/bertviz) tool by [jessevig](https://github.com/jessevig/). you can see a pre-loaded demo [here](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=twSVFOM9SopW)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Pipeline API\n",
    "\n",
    "In the previous section we saw how to load a model, in this section we see the easiest way to use HuggingFace models for inference like with the earlier examples using commercial APIs.\n",
    "\n",
    "You will see how the [HuggingFace Pipeline API](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines) perform tasks including:\n",
    "* [Text Classsification](#text-classification)\n",
    "* [Text Generation](#text-generation)\n",
    "* many more tasks [here](https://huggingface.co/tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "https://huggingface.co/tasks/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-560m\"  # https://huggingface.co/bigscience/bloom-560m\n",
    "generator = pipeline(\"text-generation\", model=model_name, device_map=\"auto\")\n",
    "\n",
    "prompt = \"To learn MLOps in 2024, start by\"\n",
    "response = generator(prompt, do_sample=False, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    f\"\"\"\n",
    "**Prompt**: {prompt}\n",
    "\n",
    "**{model_name}'s continuation**: {response[0]['generated_text']}...\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "https://huggingface.co/tasks/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More text classification models: https://huggingface.co/models?pipeline_tag=text-classification&sort=trending\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "# Create a text classification pipeline using HuggingFace transformers pipeline.\n",
    "classifier_pipe = pipeline(\"text-classification\", model=model_name)\n",
    "\n",
    "# Sample data we want to classify the sentiment of.\n",
    "sentences = [\n",
    "    \"I am feeling inspired today. What a time to be alive!\",\n",
    "    \"This talk is informative, but a bit high-level, where I can find more details?\",\n",
    "    \"I wonder about all the hype around Generative AI, is it smoke and mirrors?\",\n",
    "    \"Building production-grade machine learning systems is challenging.\",\n",
    "]\n",
    "\n",
    "# Run the pipeline!\n",
    "classifier_pipe(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
